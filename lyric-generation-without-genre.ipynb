{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T07:55:03.994793Z","iopub.status.busy":"2024-04-29T07:55:03.994052Z","iopub.status.idle":"2024-04-29T07:55:25.765515Z","shell.execute_reply":"2024-04-29T07:55:25.764657Z","shell.execute_reply.started":"2024-04-29T07:55:03.994754Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-04-29 07:55:15.575422: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-29 07:55:15.575530: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-29 07:55:15.729362: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n","from datasets import load_dataset\n","from sklearn.metrics.pairwise import cosine_similarity\n","import torch"]},{"cell_type":"markdown","metadata":{},"source":["# Load Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T07:57:20.860276Z","iopub.status.busy":"2024-04-29T07:57:20.859903Z","iopub.status.idle":"2024-04-29T07:58:50.112510Z","shell.execute_reply":"2024-04-29T07:58:50.111733Z","shell.execute_reply.started":"2024-04-29T07:57:20.860247Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"662a7b50b8e14a6e895c22bd31bcb9a0","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["dataset = load_dataset(\"amishshah/song_lyrics\")\n","dataset = dataset[\"train\"].shuffle(seed=42)\n","subset_size = 1000\n","dataset = dataset.select(range(subset_size))\n","train_test_dataset = dataset.train_test_split(test_size=0.1)\n","train_dataset = train_test_dataset[\"train\"]\n","val_dataset = train_test_dataset[\"test\"]\n","#train_test_dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n","#train_dataset = train_test_dataset[\"train\"]\n","#val_dataset = train_test_dataset[\"test\"]"]},{"cell_type":"markdown","metadata":{},"source":["# Load tokenizer and pre-trained model"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T07:59:49.888862Z","iopub.status.busy":"2024-04-29T07:59:49.888025Z","iopub.status.idle":"2024-04-29T07:59:55.313873Z","shell.execute_reply":"2024-04-29T07:59:55.312951Z","shell.execute_reply.started":"2024-04-29T07:59:49.888825Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1a7dec5885714a61ab1021014a0d0a3f","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"693d30a72d1f42a881ffbc801c20b450","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4bccf5626354e47b615f7673fb26cd2","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e3661a67b2e42cda7cf5ae8a1a0ed67","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca7ec03a83c048d481518166f4f71c79","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"36df0be00e074ccd9c444b3d23ac2582","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dcd8d4ee434b4b1585a479be68ec28a2","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n","from datasets import load_dataset\n","\n","# Load tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n","\n","# Ensure that tokenizer has padding token set\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"markdown","metadata":{},"source":["# Tokenize Dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:00:05.931482Z","iopub.status.busy":"2024-04-29T08:00:05.930493Z","iopub.status.idle":"2024-04-29T08:00:15.234369Z","shell.execute_reply":"2024-04-29T08:00:15.233387Z","shell.execute_reply.started":"2024-04-29T08:00:05.931436Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee6c101605b049dfadc2ba5603273f28","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/900 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ebcba89bcd034643bfcbd28abf717b58","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/100 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Tokenize the data\n","def tokenize_function(examples):\n","    return tokenizer(examples['lyrics'], truncation=True, padding=True, max_length=512)\n","\n","train_dataset = train_dataset.map(tokenize_function, batched=True)\n","val_dataset = val_dataset.map(tokenize_function, batched=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Fine-tuning"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:01:27.279888Z","iopub.status.busy":"2024-04-29T08:01:27.279485Z","iopub.status.idle":"2024-04-29T08:01:28.818211Z","shell.execute_reply":"2024-04-29T08:01:28.817370Z","shell.execute_reply.started":"2024-04-29T08:01:27.279857Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]}],"source":["# Set training arguments\n","training_args = TrainingArguments(\n","    output_dir='./models',\n","    num_train_epochs=4,\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=8,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=10,\n",")\n","\n","# Initialize Trainer\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset\n",")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:01:33.636142Z","iopub.status.busy":"2024-04-29T08:01:33.635450Z","iopub.status.idle":"2024-04-29T08:07:46.141276Z","shell.execute_reply":"2024-04-29T08:07:46.139912Z","shell.execute_reply.started":"2024-04-29T08:01:33.636109Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Device:  cuda\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240429_080142-g37jnr5u</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/cashel-dev/huggingface/runs/g37jnr5u' target=\"_blank\">sweet-surf-2</a></strong> to <a href='https://wandb.ai/cashel-dev/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/cashel-dev/huggingface' target=\"_blank\">https://wandb.ai/cashel-dev/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/cashel-dev/huggingface/runs/g37jnr5u' target=\"_blank\">https://wandb.ai/cashel-dev/huggingface/runs/g37jnr5u</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='452' max='452' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [452/452 05:42, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>3.449700</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>3.349200</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>3.212700</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>3.201400</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>3.221600</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>2.917700</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>3.051200</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>3.059800</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>3.006100</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>3.189100</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>2.919100</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>3.003700</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>2.912200</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>2.843900</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>2.883800</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>2.844600</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>3.021500</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>2.890300</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>2.707100</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>3.057000</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>2.823400</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>3.007700</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>2.979800</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>2.939600</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>2.776200</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>2.662100</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>2.855400</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>2.764800</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>2.879600</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>2.913300</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>2.730000</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>2.744300</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>2.849100</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>2.761900</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>2.780000</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>2.683900</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>2.680700</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>2.730200</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>2.817800</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>2.768600</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>2.738500</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>2.686900</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>2.606000</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>2.794800</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>2.833000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=452, training_loss=2.9001856398793446, metrics={'train_runtime': 372.1966, 'train_samples_per_second': 9.672, 'train_steps_per_second': 1.214, 'total_flos': 940651315200000.0, 'train_loss': 2.9001856398793446, 'epoch': 4.0})"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Check if GPU is available and if not, use CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Device: \", device)\n","\n","model.to(device)\n","# Train the model\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_path = \"./models\""]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:09:07.569432Z","iopub.status.busy":"2024-04-29T08:09:07.568428Z","iopub.status.idle":"2024-04-29T08:09:08.673851Z","shell.execute_reply":"2024-04-29T08:09:08.672694Z","shell.execute_reply.started":"2024-04-29T08:09:07.569396Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('./results/tokenizer_config.json',\n"," './results/special_tokens_map.json',\n"," './results/vocab.json',\n"," './results/merges.txt',\n"," './results/added_tokens.json')"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["model.save_pretrained(model_path)\n","tokenizer.save_pretrained(model_path)"]},{"cell_type":"markdown","metadata":{},"source":["# Lyric Generation"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:09:37.666194Z","iopub.status.busy":"2024-04-29T08:09:37.665822Z","iopub.status.idle":"2024-04-29T08:10:02.575527Z","shell.execute_reply":"2024-04-29T08:10:02.574421Z","shell.execute_reply.started":"2024-04-29T08:09:37.666164Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Complete this lyric about love and loss:  I know I was wrong\n","What I do, you need to know\n","I'm afraid you can't do your math\n","Or you wouldn't dare look up your phone\n","It's not the time to waste time\n","I'm still in this funk\n","\n","What I try to do today isn't the hard work\n","Don't you know it's never this easy\n","So sad that we're living like bandits\n","While you're lying in the grass\n","My bad things just keep getting stronger\n","Don't you know it's always gonna get better\n","\n","Losing what you can with words\n","Let's not hold you to that promise\n","\n","Do you know I tried and failed so hard in my dreams?\n","The night before my baby arrived\n","The only hope that's ever been left\n","Why does it happen\n","How I'm feeling, every day\n","Do I know what to make of you?\n","\n","Never leave a friend when I try\n","\n","I don't want to get lost, I just want to find love\n","When I look back on my life\n","Do you know how lost you were?\n","When things got so bad?\n","Don't you know I'm all lost now?\n","\n","Sometimes I just want to see the same thing\n","All night long\n","Like when we did everything together\n","When she just said \"I love you\"\n","She said \"I love you\"\n","She said \"I love you\"\n","\n","I just want to feel you again\n","All the time when you were just a girl\n","For crying in my arms\n","Baby, if she can't know you like this\n","The nights when we would stay together\n","Always together\n","Always together\n","Always together\n","My best friends and my friends\n","Can't bring themselves to believe nothing is going well\n","Don't they know it's worth trying?\n","\n","But I could tell you I could tell you all this\n","The hardest thing about you\n","You always have me believing I'm worthy even when things get so wrong\n","And you're right\n","Love your little sister, do not let anybody know\n","You know I could hold her, don't give up\n","Never let someone like you\n","Tell me you could hold her\n","She's been in those dark nights\n","The night I found love\n","When I found your baby\n","You touched my heart and made a promise\n","Your love won't fade\n","How could it last?\n","I just tried, try, try again\n","Don't you know maybe you could live to see\n","But you know I could lose a little bit of your heart\n","\n","You tried to save me a little bit of your heart\n","But you know I could lose the thing that keeps me forever\n","Oh... so lucky and sweet\n","You wanted me to stay here with you again\n","\n","We just had to get this out of the way\n","I just wanted to see you again\n","How could it last...\n","\n","\n"]}],"source":["# Load the model and tokenizer for text generation\n","from transformers import pipeline\n","\n","# Ensure your model and tokenizer are loaded correctly\n","text_generator = pipeline('text-generation', model=model_path, tokenizer=model_path)\n","\n","# Generate text using the pipeline\n","prompt = \"Complete this lyric about love and loss: \"\n","results = text_generator(prompt, max_length=600, truncation=True)\n","print(results[0]['generated_text'])\n"]},{"cell_type":"markdown","metadata":{},"source":["# Load in a fine-tuned model"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T02:47:12.387669Z","iopub.status.busy":"2024-04-29T02:47:12.386954Z","iopub.status.idle":"2024-04-29T02:47:12.617704Z","shell.execute_reply":"2024-04-29T02:47:12.616529Z","shell.execute_reply.started":"2024-04-29T02:47:12.387636Z"},"trusted":true},"outputs":[],"source":["model = GPT2LMHeadModel.from_pretrained(model_path)"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluate fine-tuning using perplexity"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T08:11:47.631700Z","iopub.status.busy":"2024-04-29T08:11:47.630977Z","iopub.status.idle":"2024-04-29T08:11:48.690803Z","shell.execute_reply":"2024-04-29T08:11:48.689635Z","shell.execute_reply.started":"2024-04-29T08:11:47.631651Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Perplexity of Pretrained Model: 981.9048461914062\n","Perplexity of Base GPT-2 Model: 989.9269409179688\n"]}],"source":["import torch\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","\n","def calculate_perplexity(model, tokenizer, text):\n","    encode = tokenizer.encode(text, return_tensors='pt')\n","    with torch.no_grad():\n","        outputs = model(encode, labels=encode)\n","        loss = outputs[0]\n","\n","    return torch.exp(loss).item()\n","\n","# Load models and tokenizer\n","model_pretrained = GPT2LMHeadModel.from_pretrained(model_path)\n","model_base = GPT2LMHeadModel.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","# Example text\n","text = \"Complete this lyric about love and loss:\"\n","\n","# Calculate perplexity\n","perplexity_pretrained = calculate_perplexity(model_pretrained, tokenizer, text)\n","perplexity_base = calculate_perplexity(model_base, tokenizer, text)\n","\n","print(f'Perplexity of Pretrained Model: {perplexity_pretrained}')\n","print(f'Perplexity of Base GPT-2 Model: {perplexity_base}')\n"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluate fine-tuning using rouge-score"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T04:22:18.632604Z","iopub.status.busy":"2024-04-29T04:22:18.632280Z","iopub.status.idle":"2024-04-29T04:22:18.974489Z","shell.execute_reply":"2024-04-29T04:22:18.973162Z","shell.execute_reply.started":"2024-04-29T04:22:18.632570Z"},"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'rouge_score'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrouge_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rouge_scorer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Example data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m generated_lyrics \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello darkness my old friend, I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve come to talk with you again\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbecause a vision softly creeping, left its seeds while I was sleeping\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m ]\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rouge_score'"]}],"source":["from rouge_score import rouge_scorer\n","\n","# Example data\n","generated_lyrics = [\n","    \"hello darkness my old friend, I've come to talk with you again\",\n","    \"because a vision softly creeping, left its seeds while I was sleeping\"\n","]\n","\n","reference_lyrics = [\n","    \"hello darkness my old friend, I've come to speak with you again\",\n","    \"because a vision softly creeping, left its seeds while I was sleeping\"\n","]\n","\n","# Initialize the ROUGE scorer, you can specify which rouge types to calculate\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","\n","# Function to calculate average scores\n","def calculate_average_rouge(generated, references):\n","    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n","    \n","    for gen, ref in zip(generated, references):\n","        score = scorer.score(ref, gen)\n","        for key in scores.keys():\n","            scores[key].append(score[key].fmeasure)\n","    \n","    average_scores = {key: sum(values) / len(values) for key, values in scores.items()}\n","    return average_scores\n","\n","# Calculate average ROUGE scores\n","average_scores = calculate_average_rouge(generated_lyrics, reference_lyrics)\n","print(\"Average ROUGE scores:\", average_scores)\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":174593070,"sourceType":"kernelVersion"},{"isSourceIdPinned":true,"modelInstanceId":34588,"sourceId":41107,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":34603,"sourceId":41124,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
