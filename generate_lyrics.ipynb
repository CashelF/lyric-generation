{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidpekar/Downloads/Python Projects/lyric generation/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"amishshah/song_lyrics\")\n",
    "dataset = dataset[\"train\"].shuffle(seed=42)\n",
    "subset_size = 10000\n",
    "dataset = dataset.select(range(subset_size))\n",
    "train_test_dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_dataset[\"train\"]\n",
    "val_dataset = train_test_dataset[\"test\"]\n",
    "#train_test_dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "#train_dataset = train_test_dataset[\"train\"]\n",
    "#val_dataset = train_test_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_subset = train_dataset.select(range(100))\n",
    "#val_subset = val_dataset.select(range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  2%|▏         | 135/9000 [23:04<2:49:26,  1.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9365, 'grad_norm': 3.0167157649993896, 'learning_rate': 4.705882352941177e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  2%|▏         | 135/9000 [41:58<2:49:26,  1.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.808, 'grad_norm': 2.2655844688415527, 'learning_rate': 4.11764705882353e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  2%|▏         | 135/9000 [1:00:59<2:49:26,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7283, 'grad_norm': 1.9488439559936523, 'learning_rate': 3.529411764705883e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  2%|▏         | 135/9000 [1:19:59<2:49:26,  1.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6544, 'grad_norm': 1.6716156005859375, 'learning_rate': 2.9411764705882354e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  2%|▏         | 135/9000 [1:38:58<2:49:26,  1.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6592, 'grad_norm': 2.3996353149414062, 'learning_rate': 2.3529411764705884e-05, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  2%|▏         | 135/9000 [1:58:00<2:49:26,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5764, 'grad_norm': 1.639112949371338, 'learning_rate': 1.7647058823529414e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  2%|▏         | 135/9000 [2:16:59<2:49:26,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.592, 'grad_norm': 2.9231934547424316, 'learning_rate': 1.1764705882352942e-05, 'epoch': 3.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  2%|▏         | 135/9000 [2:36:00<2:49:26,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5409, 'grad_norm': 2.2661397457122803, 'learning_rate': 5.882352941176471e-06, 'epoch': 3.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  2%|▏         | 135/9000 [2:54:59<2:49:26,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.547, 'grad_norm': 1.8551689386367798, 'learning_rate': 0.0, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "100%|██████████| 9000/9000 [2:50:43<00:00,  1.14s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 10243.5585, 'train_samples_per_second': 3.514, 'train_steps_per_second': 0.879, 'train_loss': 2.6714083387586807, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9000, training_loss=2.6714083387586807, metrics={'train_runtime': 10243.5585, 'train_samples_per_second': 3.514, 'train_steps_per_second': 0.879, 'total_flos': 9406513152000000.0, 'train_loss': 2.6714083387586807, 'epoch': 4.0})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Ensure that tokenizer has padding token set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = load_dataset(\"amishshah/song_lyrics\")\n",
    "dataset = dataset[\"train\"].shuffle(seed=42)\n",
    "subset_size = 10000\n",
    "dataset = dataset.select(range(subset_size))\n",
    "train_test_dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_dataset[\"train\"]\n",
    "val_dataset = train_test_dataset[\"test\"]\n",
    "\n",
    "# Load and prepare dataset\n",
    "#dataset = load_dataset(\"amishshah/song_lyrics\")\n",
    "#train_test_dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "#train_dataset = train_test_dataset[\"train\"].select(range(100))\n",
    "#val_dataset = train_test_dataset[\"test\"].select(range(100))\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['lyrics'], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=1000,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a song about the magical feeling of a first kiss under the stars.                                                                                            A song like this tells your story at the end of the day.                                                     x 4\n",
      "\n",
      "When I close my eyes and start singing\n",
      "I see a magic mirror and my mind is screaming for you again\n",
      "I am here again singing this song to remind you, the magic life, you will leave your feelings in I remember.                                   I am here again singing this song to remind you today, that you can leave them in I remember.                                                                    x 4\n",
      "When I close my eyes\n",
      "I see a magic mirror\n",
      "I have never tried to forget\n",
      "My magic mirror\n",
      "Light\n",
      "I look into your eyes and see you standing in the light\n",
      "My magic mirror\n",
      "Light\n",
      "There’s nowhere I can be\n",
      "I’m falling down\n",
      "Dark inside of my mind\n",
      "I feel such a great pain\n",
      "I am falling down\n",
      "Dark inside of my mind\n",
      "I feel such a great pain\n",
      "And I see every single thing that is inside you this day I can see it as though I can tell you the things that you can do to take over this life, not to tell you the lies that are on the other side of\n",
      "This life you are living in\n",
      "You’re living with the devil in my mind\n",
      "I was caught sleeping with the devil in my mind\n",
      "Now you’re on the other side of these worlds\n",
      "This life you are living in\n",
      "All of my life I've been going\n",
      "This life you are living in\n",
      "Yes I’m growing\n",
      "Yes I’m growing baby boom\n",
      "Yes I’m growing baby boom\n",
      "Yes we can stay together as one baby\n",
      "I was told that you could give me a man in the first\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nmodel.save_pretrained('./results')\\ntokenizer.save_pretrained('./results')\\n\\n# Load the model and tokenizer for text generation\\nfrom transformers import pipeline\\n\\n# Ensure your model and tokenizer are loaded correctly\\ndiomedes = pipeline('text-generation', model='./results', tokenizer='./results')\\n\\n# Generate text using the pipeline\\nresults = diomedes('create a love song ', max_length=600)\\nprint(results[0]['generated_text'])\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
    "\n",
    "# Define the path to the checkpoint you want to use\n",
    "checkpoint_path = './results'\n",
    "\n",
    "# Load the model and tokenizer from the checkpoint\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Ensure the tokenizer has a padding token set, if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create the text generation pipeline using the model and tokenizer from the checkpoint\n",
    "diomedes = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "results = diomedes('Create a song about the magical feeling of a first kiss under the stars. ', max_length=600)\n",
    "print(results[0]['generated_text'])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "model.save_pretrained('./results')\n",
    "tokenizer.save_pretrained('./results')\n",
    "\n",
    "# Load the model and tokenizer for text generation\n",
    "from transformers import pipeline\n",
    "\n",
    "# Ensure your model and tokenizer are loaded correctly\n",
    "diomedes = pipeline('text-generation', model='./results', tokenizer='./results')\n",
    "\n",
    "# Generate text using the pipeline\n",
    "results = diomedes('create a love song ', max_length=600)\n",
    "print(results[0]['generated_text'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Ensure that tokenizer has padding token set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Assuming the CSV file is at the path 'path_to_csv', and it includes 'tag' and 'lyrics' columns\n",
    "dataset = load_dataset(\"amishshah/song_lyrics\")\n",
    "dataset = dataset[\"train\"].shuffle(seed=42)\n",
    "subset_size = 10000  # Adjust as necessary\n",
    "dataset = dataset.select(range(subset_size))\n",
    "train_test_dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_dataset[\"train\"]\n",
    "val_dataset = train_test_dataset[\"test\"]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Prepend the tag to each lyric in the batch\n",
    "    concatenated_lyrics = [\"[Genre: \" + tag + \"] \" + lyric for tag, lyric in zip(examples[\"tag\"], examples[\"lyrics\"])]\n",
    "    return tokenizer(concatenated_lyrics, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 555/9000 [1:39:22<25:12:01, 10.74s/it]\n",
      "  1%|          | 100/9000 [01:52<2:45:14,  1.11s/it]\n",
      "  1%|          | 100/9000 [01:52<2:45:14,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1795, 'grad_norm': 5.191074371337891, 'learning_rate': 1e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 200/9000 [03:44<2:43:59,  1.12s/it]\n",
      "  2%|▏         | 200/9000 [03:44<2:43:59,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0335, 'grad_norm': 5.126195430755615, 'learning_rate': 2e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 300/9000 [05:36<2:42:10,  1.12s/it]\n",
      "  3%|▎         | 300/9000 [05:36<2:42:10,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8782, 'grad_norm': 5.482480049133301, 'learning_rate': 3e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 400/9000 [07:28<2:40:20,  1.12s/it]\n",
      "  4%|▍         | 400/9000 [07:28<2:40:20,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9091, 'grad_norm': 6.005352020263672, 'learning_rate': 4e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 500/9000 [09:20<2:38:49,  1.12s/it]\n",
      "  6%|▌         | 500/9000 [09:20<2:38:49,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8845, 'grad_norm': 6.43183708190918, 'learning_rate': 5e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 600/9000 [11:14<2:34:23,  1.10s/it]\n",
      "  7%|▋         | 600/9000 [11:14<2:34:23,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7924, 'grad_norm': 4.062376022338867, 'learning_rate': 4.9411764705882355e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 700/9000 [13:05<2:36:11,  1.13s/it]\n",
      "  8%|▊         | 700/9000 [13:05<2:36:11,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9109, 'grad_norm': 4.2698073387146, 'learning_rate': 4.882352941176471e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 800/9000 [14:56<2:33:25,  1.12s/it]\n",
      "  9%|▉         | 800/9000 [14:56<2:33:25,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7958, 'grad_norm': 2.501638174057007, 'learning_rate': 4.823529411764706e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 900/9000 [16:47<2:30:19,  1.11s/it]\n",
      " 10%|█         | 900/9000 [16:47<2:30:19,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8094, 'grad_norm': 2.873769521713257, 'learning_rate': 4.7647058823529414e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1000/9000 [18:39<2:28:25,  1.11s/it]\n",
      " 11%|█         | 1000/9000 [18:39<2:28:25,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8657, 'grad_norm': 2.8658206462860107, 'learning_rate': 4.705882352941177e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1100/9000 [20:33<2:25:25,  1.10s/it]\n",
      " 12%|█▏        | 1100/9000 [20:33<2:25:25,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7765, 'grad_norm': 2.2796690464019775, 'learning_rate': 4.647058823529412e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1200/9000 [22:24<2:23:58,  1.11s/it]\n",
      " 13%|█▎        | 1200/9000 [22:24<2:23:58,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8443, 'grad_norm': 2.3918046951293945, 'learning_rate': 4.588235294117647e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1300/9000 [24:15<2:22:17,  1.11s/it]\n",
      " 14%|█▍        | 1300/9000 [24:15<2:22:17,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8646, 'grad_norm': 2.18247652053833, 'learning_rate': 4.5294117647058826e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1400/9000 [26:06<2:21:23,  1.12s/it]\n",
      " 16%|█▌        | 1400/9000 [26:06<2:21:23,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7581, 'grad_norm': 1.7247874736785889, 'learning_rate': 4.470588235294118e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1500/9000 [28:07<2:21:15,  1.13s/it]\n",
      " 17%|█▋        | 1500/9000 [28:07<2:21:15,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7235, 'grad_norm': 2.5414185523986816, 'learning_rate': 4.411764705882353e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1600/9000 [30:02<2:18:22,  1.12s/it]\n",
      " 18%|█▊        | 1600/9000 [30:02<2:18:22,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7131, 'grad_norm': 1.8392738103866577, 'learning_rate': 4.3529411764705885e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1700/9000 [31:54<2:15:11,  1.11s/it]\n",
      " 19%|█▉        | 1700/9000 [31:54<2:15:11,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.751, 'grad_norm': 1.9378267526626587, 'learning_rate': 4.294117647058823e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1800/9000 [33:46<2:14:56,  1.12s/it]\n",
      " 20%|██        | 1800/9000 [33:46<2:14:56,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7167, 'grad_norm': 1.8970435857772827, 'learning_rate': 4.235294117647059e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 1900/9000 [35:37<2:11:12,  1.11s/it]\n",
      " 21%|██        | 1900/9000 [35:37<2:11:12,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7823, 'grad_norm': 1.760888934135437, 'learning_rate': 4.1764705882352944e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2000/9000 [37:28<2:11:01,  1.12s/it]\n",
      " 22%|██▏       | 2000/9000 [37:28<2:11:01,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7613, 'grad_norm': 2.151956796646118, 'learning_rate': 4.11764705882353e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2100/9000 [39:22<2:09:17,  1.12s/it]\n",
      " 23%|██▎       | 2100/9000 [39:22<2:09:17,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7527, 'grad_norm': 2.0038342475891113, 'learning_rate': 4.058823529411765e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 2200/9000 [41:14<2:06:05,  1.11s/it]\n",
      " 24%|██▍       | 2200/9000 [41:14<2:06:05,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7848, 'grad_norm': 1.525112271308899, 'learning_rate': 4e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 2300/9000 [43:08<2:13:44,  1.20s/it]\n",
      " 26%|██▌       | 2300/9000 [43:08<2:13:44,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8023, 'grad_norm': 2.02986478805542, 'learning_rate': 3.9411764705882356e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2400/9000 [45:03<2:06:05,  1.15s/it]\n",
      " 27%|██▋       | 2400/9000 [45:03<2:06:05,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6343, 'grad_norm': 2.471766710281372, 'learning_rate': 3.882352941176471e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 2500/9000 [46:57<2:03:03,  1.14s/it]\n",
      " 28%|██▊       | 2500/9000 [46:57<2:03:03,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7019, 'grad_norm': 1.4912142753601074, 'learning_rate': 3.8235294117647055e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 2600/9000 [48:53<1:59:55,  1.12s/it]\n",
      " 29%|██▉       | 2600/9000 [48:53<1:59:55,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6765, 'grad_norm': 1.479359745979309, 'learning_rate': 3.7647058823529415e-05, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 2700/9000 [50:47<1:57:16,  1.12s/it]\n",
      " 30%|███       | 2700/9000 [50:47<1:57:16,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6146, 'grad_norm': 1.7265595197677612, 'learning_rate': 3.705882352941177e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 2800/9000 [52:41<1:57:50,  1.14s/it]\n",
      " 31%|███       | 2800/9000 [52:41<1:57:50,  1.14s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.633, 'grad_norm': 2.0263445377349854, 'learning_rate': 3.6470588235294114e-05, 'epoch': 1.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 2900/9000 [54:35<1:56:54,  1.15s/it]\n",
      " 32%|███▏      | 2900/9000 [54:35<1:56:54,  1.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6509, 'grad_norm': 2.2399399280548096, 'learning_rate': 3.5882352941176474e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3000/9000 [56:28<1:51:54,  1.12s/it]\n",
      " 33%|███▎      | 3000/9000 [56:28<1:51:54,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6395, 'grad_norm': 1.793200969696045, 'learning_rate': 3.529411764705883e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 3100/9000 [58:23<1:49:47,  1.12s/it]\n",
      " 34%|███▍      | 3100/9000 [58:23<1:49:47,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6032, 'grad_norm': 1.782893419265747, 'learning_rate': 3.470588235294118e-05, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 3200/9000 [1:00:14<1:49:11,  1.13s/it]\n",
      " 36%|███▌      | 3200/9000 [1:00:14<1:49:11,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6524, 'grad_norm': 1.8332531452178955, 'learning_rate': 3.411764705882353e-05, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 3300/9000 [1:02:07<1:47:07,  1.13s/it]\n",
      " 37%|███▋      | 3300/9000 [1:02:07<1:47:07,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6657, 'grad_norm': 2.1436023712158203, 'learning_rate': 3.352941176470588e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3400/9000 [1:03:59<1:44:47,  1.12s/it]\n",
      " 38%|███▊      | 3400/9000 [1:03:59<1:44:47,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6244, 'grad_norm': 1.6253551244735718, 'learning_rate': 3.294117647058824e-05, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 3500/9000 [1:05:52<1:43:07,  1.12s/it]\n",
      " 39%|███▉      | 3500/9000 [1:05:52<1:43:07,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6421, 'grad_norm': 2.2136332988739014, 'learning_rate': 3.235294117647059e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 3600/9000 [1:07:46<1:41:28,  1.13s/it]\n",
      " 40%|████      | 3600/9000 [1:07:46<1:41:28,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5957, 'grad_norm': 1.8594571352005005, 'learning_rate': 3.176470588235294e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 3700/9000 [1:09:37<1:37:42,  1.11s/it]\n",
      " 41%|████      | 3700/9000 [1:09:37<1:37:42,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5908, 'grad_norm': 1.4845479726791382, 'learning_rate': 3.11764705882353e-05, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 3800/9000 [1:11:27<1:34:42,  1.09s/it]\n",
      " 42%|████▏     | 3800/9000 [1:11:27<1:34:42,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6166, 'grad_norm': 1.806246280670166, 'learning_rate': 3.058823529411765e-05, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3900/9000 [1:13:18<1:32:54,  1.09s/it]\n",
      " 43%|████▎     | 3900/9000 [1:13:18<1:32:54,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6256, 'grad_norm': 1.546592354774475, 'learning_rate': 3e-05, 'epoch': 1.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4000/9000 [1:15:08<1:30:56,  1.09s/it]\n",
      " 44%|████▍     | 4000/9000 [1:15:08<1:30:56,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5553, 'grad_norm': 1.5444260835647583, 'learning_rate': 2.9411764705882354e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 4100/9000 [1:17:01<1:30:25,  1.11s/it]\n",
      " 46%|████▌     | 4100/9000 [1:17:01<1:30:25,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6342, 'grad_norm': 1.5049240589141846, 'learning_rate': 2.8823529411764703e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 4200/9000 [1:18:52<1:28:09,  1.10s/it]\n",
      " 47%|████▋     | 4200/9000 [1:18:52<1:28:09,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6398, 'grad_norm': 1.5949453115463257, 'learning_rate': 2.823529411764706e-05, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 4300/9000 [1:20:44<1:27:30,  1.12s/it]\n",
      " 48%|████▊     | 4300/9000 [1:20:44<1:27:30,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.671, 'grad_norm': 1.9855999946594238, 'learning_rate': 2.7647058823529416e-05, 'epoch': 1.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 4400/9000 [1:22:36<1:24:49,  1.11s/it]\n",
      " 49%|████▉     | 4400/9000 [1:22:36<1:24:49,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7205, 'grad_norm': 1.6320282220840454, 'learning_rate': 2.7058823529411766e-05, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4500/9000 [1:24:27<1:23:35,  1.11s/it]\n",
      " 50%|█████     | 4500/9000 [1:24:27<1:23:35,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6107, 'grad_norm': 1.5965609550476074, 'learning_rate': 2.647058823529412e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 4600/9000 [1:26:19<1:20:49,  1.10s/it]\n",
      " 51%|█████     | 4600/9000 [1:26:19<1:20:49,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5812, 'grad_norm': 1.6807786226272583, 'learning_rate': 2.5882352941176475e-05, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 4700/9000 [1:29:13<1:27:02,  1.21s/it] \n",
      " 52%|█████▏    | 4700/9000 [1:29:13<1:27:02,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.616, 'grad_norm': 1.6068819761276245, 'learning_rate': 2.5294117647058825e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 4800/9000 [1:31:05<1:17:55,  1.11s/it]\n",
      " 53%|█████▎    | 4800/9000 [1:31:05<1:17:55,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.556, 'grad_norm': 1.577765703201294, 'learning_rate': 2.4705882352941178e-05, 'epoch': 2.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 4900/9000 [1:32:57<1:16:32,  1.12s/it]\n",
      " 54%|█████▍    | 4900/9000 [1:32:57<1:16:32,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5978, 'grad_norm': 1.6630079746246338, 'learning_rate': 2.411764705882353e-05, 'epoch': 2.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5000/9000 [1:34:48<1:14:51,  1.12s/it]\n",
      " 56%|█████▌    | 5000/9000 [1:34:48<1:14:51,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5958, 'grad_norm': 2.3526158332824707, 'learning_rate': 2.3529411764705884e-05, 'epoch': 2.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 5100/9000 [1:36:42<1:13:12,  1.13s/it]\n",
      " 57%|█████▋    | 5100/9000 [1:36:42<1:13:12,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5709, 'grad_norm': 1.7172824144363403, 'learning_rate': 2.2941176470588237e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 5200/9000 [1:38:33<1:10:17,  1.11s/it]\n",
      " 58%|█████▊    | 5200/9000 [1:38:33<1:10:17,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5832, 'grad_norm': 1.9749181270599365, 'learning_rate': 2.235294117647059e-05, 'epoch': 2.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 5300/9000 [1:40:25<1:08:27,  1.11s/it]\n",
      " 59%|█████▉    | 5300/9000 [1:40:25<1:08:27,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5287, 'grad_norm': 1.8011837005615234, 'learning_rate': 2.1764705882352943e-05, 'epoch': 2.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 5400/9000 [1:42:17<1:06:31,  1.11s/it]\n",
      " 60%|██████    | 5400/9000 [1:42:17<1:06:31,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5467, 'grad_norm': 1.7921557426452637, 'learning_rate': 2.1176470588235296e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 5500/9000 [1:44:08<1:04:49,  1.11s/it]\n",
      " 61%|██████    | 5500/9000 [1:44:08<1:04:49,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4961, 'grad_norm': 2.307626485824585, 'learning_rate': 2.058823529411765e-05, 'epoch': 2.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 5600/9000 [1:46:01<1:03:13,  1.12s/it]\n",
      " 62%|██████▏   | 5600/9000 [1:46:01<1:03:13,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.547, 'grad_norm': 1.761753797531128, 'learning_rate': 2e-05, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 5700/9000 [1:47:53<1:01:15,  1.11s/it]\n",
      " 63%|██████▎   | 5700/9000 [1:47:53<1:01:15,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5348, 'grad_norm': 2.071650743484497, 'learning_rate': 1.9411764705882355e-05, 'epoch': 2.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 5800/9000 [1:49:44<59:31,  1.12s/it]  \n",
      " 64%|██████▍   | 5800/9000 [1:49:44<59:31,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5471, 'grad_norm': 1.7836315631866455, 'learning_rate': 1.8823529411764708e-05, 'epoch': 2.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 5900/9000 [1:51:36<57:19,  1.11s/it]\n",
      " 66%|██████▌   | 5900/9000 [1:51:36<57:19,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5284, 'grad_norm': 1.9840792417526245, 'learning_rate': 1.8235294117647057e-05, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6000/9000 [1:53:28<55:42,  1.11s/it]\n",
      " 67%|██████▋   | 6000/9000 [1:53:28<55:42,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5222, 'grad_norm': 1.5470770597457886, 'learning_rate': 1.7647058823529414e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 6100/9000 [1:55:21<54:00,  1.12s/it]  \n",
      " 68%|██████▊   | 6100/9000 [1:55:21<54:00,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5913, 'grad_norm': 1.9670213460922241, 'learning_rate': 1.7058823529411767e-05, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 6200/9000 [1:57:13<51:54,  1.11s/it]\n",
      " 69%|██████▉   | 6200/9000 [1:57:13<51:54,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5543, 'grad_norm': 2.285857915878296, 'learning_rate': 1.647058823529412e-05, 'epoch': 2.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 6300/9000 [1:59:04<50:00,  1.11s/it]\n",
      " 70%|███████   | 6300/9000 [1:59:04<50:00,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5544, 'grad_norm': 1.6880210638046265, 'learning_rate': 1.588235294117647e-05, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 6400/9000 [2:00:55<48:21,  1.12s/it]\n",
      " 71%|███████   | 6400/9000 [2:00:55<48:21,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5407, 'grad_norm': 2.2980682849884033, 'learning_rate': 1.5294117647058826e-05, 'epoch': 2.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 6500/9000 [2:02:47<46:41,  1.12s/it]\n",
      " 72%|███████▏  | 6500/9000 [2:02:47<46:41,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6081, 'grad_norm': 1.949706792831421, 'learning_rate': 1.4705882352941177e-05, 'epoch': 2.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 6600/9000 [2:05:59<44:33,  1.11s/it]   \n",
      " 73%|███████▎  | 6600/9000 [2:05:59<44:33,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5451, 'grad_norm': 1.6758157014846802, 'learning_rate': 1.411764705882353e-05, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 6700/9000 [2:07:50<42:31,  1.11s/it]\n",
      " 74%|███████▍  | 6700/9000 [2:07:50<42:31,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6132, 'grad_norm': 2.5975587368011475, 'learning_rate': 1.3529411764705883e-05, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 6800/9000 [2:09:42<41:10,  1.12s/it]\n",
      " 76%|███████▌  | 6800/9000 [2:09:42<41:10,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.522, 'grad_norm': 1.999974012374878, 'learning_rate': 1.2941176470588238e-05, 'epoch': 3.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 6900/9000 [2:11:34<38:55,  1.11s/it]\n",
      " 77%|███████▋  | 6900/9000 [2:11:34<38:55,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5106, 'grad_norm': 1.6577931642532349, 'learning_rate': 1.2352941176470589e-05, 'epoch': 3.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7000/9000 [2:13:26<37:10,  1.12s/it]\n",
      " 78%|███████▊  | 7000/9000 [2:13:26<37:10,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.525, 'grad_norm': 2.8586227893829346, 'learning_rate': 1.1764705882352942e-05, 'epoch': 3.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 7100/9000 [2:15:20<35:19,  1.12s/it]\n",
      " 79%|███████▉  | 7100/9000 [2:15:20<35:19,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5042, 'grad_norm': 1.9048960208892822, 'learning_rate': 1.1176470588235295e-05, 'epoch': 3.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 7200/9000 [2:17:12<33:44,  1.12s/it]\n",
      " 80%|████████  | 7200/9000 [2:17:12<33:44,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5636, 'grad_norm': 1.7651855945587158, 'learning_rate': 1.0588235294117648e-05, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 7300/9000 [2:19:03<31:30,  1.11s/it]\n",
      " 81%|████████  | 7300/9000 [2:19:03<31:30,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4767, 'grad_norm': 1.9282810688018799, 'learning_rate': 1e-05, 'epoch': 3.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 7400/9000 [2:20:55<29:39,  1.11s/it]\n",
      " 82%|████████▏ | 7400/9000 [2:20:55<29:39,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4754, 'grad_norm': 1.7005842924118042, 'learning_rate': 9.411764705882354e-06, 'epoch': 3.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 7500/9000 [2:22:47<27:49,  1.11s/it]\n",
      " 83%|████████▎ | 7500/9000 [2:22:47<27:49,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.502, 'grad_norm': 1.8260676860809326, 'learning_rate': 8.823529411764707e-06, 'epoch': 3.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 7600/9000 [2:24:41<26:06,  1.12s/it]\n",
      " 84%|████████▍ | 7600/9000 [2:24:41<26:06,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4882, 'grad_norm': 1.4478967189788818, 'learning_rate': 8.23529411764706e-06, 'epoch': 3.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 7700/9000 [2:26:32<24:08,  1.11s/it]\n",
      " 86%|████████▌ | 7700/9000 [2:26:32<24:08,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4966, 'grad_norm': 1.9711883068084717, 'learning_rate': 7.647058823529413e-06, 'epoch': 3.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 7800/9000 [2:28:24<22:20,  1.12s/it]\n",
      " 87%|████████▋ | 7800/9000 [2:28:24<22:20,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4859, 'grad_norm': 1.778118371963501, 'learning_rate': 7.058823529411765e-06, 'epoch': 3.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7900/9000 [2:30:16<20:35,  1.12s/it]\n",
      " 88%|████████▊ | 7900/9000 [2:30:16<20:35,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5081, 'grad_norm': 2.2681140899658203, 'learning_rate': 6.470588235294119e-06, 'epoch': 3.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8000/9000 [2:32:08<18:40,  1.12s/it]\n",
      " 89%|████████▉ | 8000/9000 [2:32:08<18:40,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5537, 'grad_norm': 2.2819535732269287, 'learning_rate': 5.882352941176471e-06, 'epoch': 3.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 8100/9000 [2:34:02<16:54,  1.13s/it]\n",
      " 90%|█████████ | 8100/9000 [2:34:02<16:54,  1.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4968, 'grad_norm': 1.3971041440963745, 'learning_rate': 5.294117647058824e-06, 'epoch': 3.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 8200/9000 [2:35:54<14:51,  1.11s/it]\n",
      " 91%|█████████ | 8200/9000 [2:35:54<14:51,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5436, 'grad_norm': 1.7098110914230347, 'learning_rate': 4.705882352941177e-06, 'epoch': 3.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 8300/9000 [2:37:46<13:03,  1.12s/it]\n",
      " 92%|█████████▏| 8300/9000 [2:37:46<13:03,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4652, 'grad_norm': 1.8977075815200806, 'learning_rate': 4.11764705882353e-06, 'epoch': 3.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 8400/9000 [2:39:38<11:11,  1.12s/it]\n",
      " 93%|█████████▎| 8400/9000 [2:39:38<11:11,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5197, 'grad_norm': 2.176227331161499, 'learning_rate': 3.5294117647058825e-06, 'epoch': 3.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 8500/9000 [2:41:29<09:14,  1.11s/it]\n",
      " 94%|█████████▍| 8500/9000 [2:41:29<09:14,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4874, 'grad_norm': 2.0052096843719482, 'learning_rate': 2.9411764705882355e-06, 'epoch': 3.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 8600/9000 [2:43:23<07:28,  1.12s/it]\n",
      " 96%|█████████▌| 8600/9000 [2:43:23<07:28,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.541, 'grad_norm': 1.7421655654907227, 'learning_rate': 2.3529411764705885e-06, 'epoch': 3.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 8700/9000 [2:45:15<05:33,  1.11s/it]\n",
      " 97%|█████████▋| 8700/9000 [2:45:15<05:33,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5566, 'grad_norm': 1.6076319217681885, 'learning_rate': 1.7647058823529412e-06, 'epoch': 3.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 8800/9000 [2:47:07<03:42,  1.11s/it]\n",
      " 98%|█████████▊| 8800/9000 [2:47:07<03:42,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5383, 'grad_norm': 2.3624112606048584, 'learning_rate': 1.1764705882352942e-06, 'epoch': 3.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 8900/9000 [2:48:58<01:51,  1.12s/it]\n",
      " 99%|█████████▉| 8900/9000 [2:48:59<01:51,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4903, 'grad_norm': 1.7136634588241577, 'learning_rate': 5.882352941176471e-07, 'epoch': 3.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9000/9000 [2:50:50<00:00,  1.12s/it]\n",
      "100%|██████████| 9000/9000 [2:50:50<00:00,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4696, 'grad_norm': 1.7365777492523193, 'learning_rate': 0.0, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 9000/9000 [2:50:52<00:00,  1.14s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 10252.5589, 'train_samples_per_second': 3.511, 'train_steps_per_second': 0.878, 'train_loss': 2.635204077826606, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9000, training_loss=2.635204077826606, metrics={'train_runtime': 10252.5589, 'train_samples_per_second': 3.511, 'train_steps_per_second': 0.878, 'total_flos': 9406513152000000.0, 'train_loss': 2.635204077826606, 'epoch': 4.0})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Genre: rap]  BEST ART IN THE WORLD [Art: rap]  YOU NEED TO BE HAVING THIS [art: rap]  IF I HAVE A LOT OF PEOPLE [art: rap] I'm gonna be the big one [art: rap] WHAT ARE YOU CALLING AN ECHOSETHER BITCH [art: rap] THIS RATE NEEDS TO BE SACRED WITH SHIT [art: rap] YAY [art: rap] NINE STREETS AVAILABLE IN BLACK ONLY [art: rap] I GOT NO TRUTH TO THAT, FUCKING KIND OF THIS [arts: rap] SICK OF MENTALLY CIVILIZED PEOPLE [arts: rap] Fucking FAGGED BITCHED IN A WOOF WHO DONT CANT CARE [art: rap] I AM NOT TRULY A FAG [art: rap] YOU COULD BITCH MY STEDGE TICKS [ART: rap] WHAT ARE YOU DOING HERE? [art: rap] WHAT A DEATH IS HUW? [art: rap] BITCHY STUCK IN A KID WITH THE FUCKING BITCH [art: rap] YOU SHOULD GET SAVED [art: rap] GET THE FUCK OUT [art: rap] YEAH [art: rap] YOU WANT A BITCH? [art: rap] YEAH [art: rap] DONT EVEN SCREAM [art: rap] YOU WONT SUE ME I WANT A FUCKING BITCH IN THE BUNNY [art: rap] HAAAAAAAAAAAAAAAAAAAS [art: rap] WOOOOOOOOOOOOOOOOOOOOOOOOO [art: rap] STOP FUCKING STUFF YOU BLOW [art: rap] YEAH HAAAAAAAAAMWAAAS [art: rap] YOU WANT ONE THAT BITCHES TO FUCK WITH HIS BRAIN? [art: rap] YES [art: rap] YOU SHOULDN'T MAKE BUDGETS HEARS [art: rap] HAWK [art: rap] STOP FUCKING CHEF THINGS YOU'RE DOING NOOOOOOOOOOO [art: all rap] YEAH FUCK YOU SICK OF YOUR OWN SEXUALITY [art: rap] YEAH FUCKY BITCH I DON'T WANT BITCHES [art: rap] STOP FUCKING KIND OF THE LOVE [art: rap] NOOOOOOOOOOO [art: rap] [art: music music] [art: music music] [art: music music] [art: music music] [art: music music] [art: music music] [art: music music] [art: music music] [art: music music] [art: music music] [art: music music] [art: music music] [art: music music] [art:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./results')\n",
    "tokenizer.save_pretrained('./results')\n",
    "\n",
    "# Load the model and tokenizer for text generation\n",
    "from transformers import pipeline\n",
    "\n",
    "# Ensure your model and tokenizer are loaded correctly\n",
    "diomedes = pipeline('text-generation', model='./results', tokenizer='./results')\n",
    "\n",
    "# Example of generating genre-specific text\n",
    "genre = \"rap\"  # Replace with any genre present in your dataset\n",
    "prompt = f\"[Genre: {genre}] \"\n",
    "results = diomedes(prompt, max_length=600)\n",
    "print(results[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Genre: pop] 하어말를 야랥 덴언... It was a pretty cool video; the sound of the song went through a very special period as one would expect. It looks quite classic from the beginning of our recording (like a little song by \"Wah! Yui!\"). From the end of \"I Like My Voice\" onwards with \"I Won't Stop Losing You\" (\"I Won't Do That\"), the sounds of the song got more prominent. The song sounds like it was inspired by a Japanese song known as \"Ima Wanna Go\", which was written in the 1940s and in which an 8-inch tape of the same song was distributed. The \"Wah!\" song sounds like it could be very effective in a way for recording an old song, because as we mentioned, it was played from time to time - one day there may be a song that reminds one to \"go\" after another one, and this will lead one to thinking that \"Go\" was made by \"Wah!\" before the \"Wah!\" album came out. So \"Wah!\" is still a popular song from this era. The song is not only very popular because it has a very unusual sound and it's very very popular (laughs). The best thing that we achieved, for us, was to create something that is unique and unique - the video that we made that was \"I Got You.\" Because of the way in which it developed as an album of \"I Got You,\" we wanted to make \"I Got You\" for the first time in our lives. The sound and lyrics of \"I Got You\" changed drastically after that. So it became important to us to create a music video that is both unique and special - how to use the video and what to do with it. So the first time, \"I Got You\" comes out, we started recording our own songs and songs made using \"I Got You\": the original \"Wah!\" and the new \"I Got You\" albums were recorded after that. \"I Got You\" by \"Wah!\" was the original song made that were written after \"I Got You.\" Although we recorded many songs, we did not write anything, and the whole sound of \"Mm-hooooo-uh\" was made using \"A-h-h-t\" as sound. \"F-H-T\"? \"F-h-t, I gotta kill you, it gonna make me happy\" is the original song created because \"I Got You\" did not have any sound from the original \"I Got You.\" So it has changed a lot since then.\n",
      "\n",
      "In the next \"Wah! Yui!\" we will talk about the lyrics, which will be interesting for us. We wrote these things after listening to the songs \"Doo Ya Bawd\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
    "\n",
    "# Define the path to the checkpoint you want to use\n",
    "checkpoint_path = './results'\n",
    "\n",
    "# Load the model and tokenizer from the checkpoint\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Ensure the tokenizer has a padding token set, if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create the text generation pipeline using the model and tokenizer from the checkpoint\n",
    "diomedes = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Now you can generate text using the pipeline with your fine-tuned model\n",
    "genre = \"pop\"  # Replace with any genre present in your dataset\n",
    "prompt = f\"[Genre: {genre}] \"  # Prompt includes a genre-specific tag\n",
    "results = diomedes(prompt, max_length=600)\n",
    "print(results[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a song about the feeling of returning home after many years:  Happiness and happiness are usually associated with more than just the release of our spirits; they also occur in the form of a combination of sadness and pleasure. People do seem to have a tendency to want something back—happiness, energy, self-esteem, and joy—whether those things are in the form of a new record or not; these feelings (and their accompanying feelings of relief and relief, as well as feelings of being happy and relieved) are what allow our spirits to return to their original sense of home. The release of joy and happiness was seen in the form of singing a song about returning to that sense of home; it was not in isolation by the listeners: instead people seemed to be singing about a return to home they hadn't been able to come home with. Again, the way we approach the release of joy and happiness is similar to those that might occur on the release of sadness: we think of the emotion of returning to a sense of home after many years, but it is also more likely to be associated with a return to the spirit state after many years. That is why we should avoid singing or singing about sad or unpleasant feelings about the release of joy and happiness; instead we should consider them as personal experiences rather than as being a kind of personal response. A good way to handle them in this way is to listen to them as an emotional experience: listen to them as part of our experience with the release of joy and happiness.\n",
      "The release of joy and happiness was seen in the form of singing a song about returning to that sense of home; it was not in isolation by the listeners: instead people seemed to be singing about a return to home they hadn't been able to come home with. Again, the way we approach the release of joy and happiness is similar to those that might occur on the release of sadness: we think of the emotion of returning to a sense of home after many years, but it is also more likely to be associated with a return to the spirit state after many years. That is why we should avoid singing or singing about sad or unpleasant feelings about the release of joy and happiness; instead we should consider them as personal experiences rather than as being a kind of personal response. A good way to handle them in this way is to listen to them as an emotional experience: listen to them as part of our experience with the release of joy and happiness.\n",
      "Eating a bowl of rice often brings back the memories of the life you had in your old life. A common mistake that comes up when discussing the release of joy and happiness is the misconception that eating a bowl of vegetables does not make you aware of the state of our heart and soul that we would be in if we lived in a different place, or if we ate in many different ways. It is more common to talk about eating a bowl of rice or eating a banana over coffee than eating a\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Ensure the tokenizer has a padding token set, which is necessary for some configurations\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model and tokenizer for text generation\n",
    "diomedes = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Simple prompt\n",
    "prompt = \"Create a song about the magical feeling of a first kiss under the stars: \"\n",
    "\n",
    "# Generate text using the pipeline\n",
    "results = diomedes(prompt, max_length=600)\n",
    "print(results[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
