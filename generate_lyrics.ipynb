{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialization and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code imports the necessary libraries and modules. The ‘transformers’ module provides models and utilities for working with models like GPT-2. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model and Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code initializes the tokenizer and model with the pre-trained GPT-2 parameters. Additionally, a padding token is set to indicate the end of a text sequence. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"amishshah/song_lyrics\")\n",
    "dataset = dataset[\"train\"].shuffle(seed=42)\n",
    "subset_size = 10000\n",
    "dataset = dataset.select(range(subset_size))\n",
    "train_test_dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_dataset[\"train\"]\n",
    "val_dataset = train_test_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of code the dataset is loaded. A subset of the data (10,000 songs)  is chosen and shuffled. This reduces training time and ensures that all genres of music are used in training. Then the dataset is split into training and validation sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['lyrics'], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code takes the training and validation datasets and converts them into a format the model can process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=1000,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code sets the training arguments and initializes a Trainer object with the model, training arguments, data collator, and datasets. For the training arguments, the results of the training are put into the output directory and the trainer does four passes over the dataset. The weight_decay parameter adds a regularization term to prevent the model from fitting the training data too closely. Finally, the progress will be tracked every one thousand steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starts the training process using the previously setup parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a song about overcoming a major challenge. \n",
      "\n",
      "You know some people can’t stand the fact that somebody is losing\n",
      "And I think I’m in a class, I have my reasons why\n",
      "Is a stranger coming for you or is he? \n",
      "\n",
      "That all goes without saying\n",
      "I can say this many times\n",
      "Don’t want to tell you many truths\n",
      "You don’t wanna say anything\n",
      "It seems so simple and basic\n",
      "But these days I’m too accustomed to be listening to\n",
      "That everything else has gone unsaid\n",
      "\n",
      "Then I’m ready to say the\n",
      "Lord's Prayer\n",
      "And let us pray for Him with all our heart\n",
      "In His name\n",
      "He will do me good.\n",
      "I’ll help you with your other problems\n",
      "He will make me feel my way to heaven\n",
      "Come on around and save your man\n",
      "Let them put down your sins\n",
      "So forgive your enemies, be kind to yourself.\n",
      "Let us all pray for our fellow man.\n",
      "I’ll help them with their other problems\n",
      "Come on around and save your man\n",
      "Let us all pray for His name and forgive me with all of our heart\n",
      "\n",
      "I hear a call that you’re standing right outside\n",
      "I see, I hear the voice of your Savior coming at your door\n",
      "Just a little closer\n",
      "So let us be gentle\n",
      "Let us be true lovers\n",
      "And do us good together\n",
      "Because we know just how to live together\n",
      "\n",
      "The night will come when every human being\n",
      "Loves God together\n",
      "\n",
      "And He’s here with a promise to redeem His people\n",
      "If You love them one another\n",
      "I’ll love you forever and ever\n",
      "This world needs you very well\n",
      "And I’ma always be your guide, a guide for all of your people\n",
      "\n",
      "I know, oh Lord, I feel the same way\n",
      "And You’re in the right place\n",
      "There’s nothing more good\n",
      "Just know that the Lord can’t be without me\n",
      "Don’t ask me why I make You my guide..\n",
      "\n",
      "And I ask again and again\n",
      "I hear a call that I remember coming\n",
      "And It is Jesus who came and took away my sins\n",
      "And He asked Me to heal them\n",
      "I am His one and only guide\n",
      "Don’t ask, ask or tell too many lies\n",
      "No, it’s Jesus who came and took away my sins\n",
      "And He asked Me to heal them\n",
      "I am His one and only guide\n",
      "Don’t ask, and it’s Jesus who came and took away my sins\n",
      "And He asked me to heal them\n",
      "I am his one and only guide..\n",
      "\n",
      "I hear a call\n",
      "That I heard in my dreams\n",
      "That you’re praying to save Your man\n",
      "When even though You’re not my friend\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
    "checkpoint_path = './results'\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "results = text_generator('Write a song about overcoming a major challenge. ', max_length=600)\n",
    "print(results[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, it can be used to generate lyrics! The fine-tuned model is loaded and the user can enter a prompt with a specified character limit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare to Base GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a song about overcoming a major challenge.  For example, let's call a song to be played on Friday nights. If The Beatles have played The Beatles, so do let that song have its same lead-up chorus.  Or say, Let's take The Beatles perform in the same place as The Doors performed in '67.  Then the songs would be called Let Them Dance and It would be a big song, except let's call The Doors in that place.  Let's assume that the lead-up chorus is as loud as The Doors used to be.  It's an extra six feet.  The lead-up song might have five songs.  These five would then each go through six separate pieces.  But the lead-up song, even if two songs are included, would still end in the same song.  The Beatles could have performed at 7:00PM or 11:00PM, at or close to midnight.  Let's put that number to a scale like 4-10. (The Beatles had the top four best-known songs of all time in 1967.)\n",
      "That's where you leave your \"The Beatles: Part 1 Songs Song\" to the Beatles: Part 2 (to be completed.)\n",
      "And to be fair, that can be a useful part of a book, especially considering that they were doing it so young:  But why should anyone care?  Here's what's been found:\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "gpt_text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "prompt = \"Write a song about overcoming a major challenge. \"\n",
    "results = gpt_text_generator(prompt, max_length=600)\n",
    "print(results[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate using ChatGPT 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='It is subjective and depends on personal preference, but based on the provided excerpts, Lyrics B seem to be more emotionally powerful and cohesive.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"OPENAI_API_KEY\",\n",
    ")\n",
    "\n",
    "def compare_lyrics(lyrics1, lyrics2):\n",
    "    prompt_text = f\"Here are two sets of song lyrics:\\n\\nLyrics A:\\n{lyrics1}\\n\\nLyrics B:\\n{lyrics2}\\n\\nWhich set of lyrics do you think is better?\"\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_text,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "\n",
    "#     print(response.choices[0].text.strip())\n",
    "    print(chat_completion.choices[0].message)\n",
    "    \n",
    "    \n",
    "prompt = \"Complete this lyric about love and loss:\"\n",
    "# Load models and tokenizer\n",
    "model_base = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "checkpoint_path = './results'\n",
    "model_fine_tuned = GPT2LMHeadModel.from_pretrained(checkpoint_path)\n",
    "tokenizer_fine_tuned = GPT2Tokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "text_generator_finetuned = pipeline('text-generation', model=model_fine_tuned, tokenizer=tokenizer_fine_tuned)\n",
    "generated_lyrics_finetuned = text_generator_finetuned(prompt, max_length=500, truncation=True)[0]['generated_text']\n",
    "\n",
    "text_generator_base = pipeline('text-generation', model=model_base, tokenizer=tokenizer)\n",
    "generated_lyrics_base = text_generator_base(prompt, max_length=500, truncation=True)[0]['generated_text']\n",
    "\n",
    "# Call the function to compare the lyrics\n",
    "compare_lyrics(generated_lyrics_base, generated_lyrics_finetuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Using A Loop To Compare and Find Percentage of the Time That The Fine Tuned Model is prefered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_prompts = [\n",
    "    \"Write a song about finding love in unexpected places.\",\n",
    "    \"Write a song about the feeling of losing a close friend.\",\n",
    "    \"Write a song about memories of summer evenings.\",\n",
    "    \"Write a song about the journey of self-discovery.\",\n",
    "    \"Write a song about the first snowfall of the year.\",\n",
    "    \"Write a song about overcoming a major challenge.\",\n",
    "    \"Write a song about dreams of a better future.\",\n",
    "    \"Write a song about moments of peaceful solitude.\",\n",
    "    \"Write a song about the thrill of a new adventure.\",\n",
    "    \"Write a song about reflections on past mistakes.\",\n",
    "    \"Write a song about celebrating a major achievement.\",\n",
    "    \"Write a song about the magic of childhood imaginations.\",\n",
    "    \"Write a song about long drives on scenic roads.\",\n",
    "    \"Write a song about the pain of unrequited love.\",\n",
    "    \"Write a song about the warmth of coming home.\",\n",
    "    \"Write a song about nights spent under the stars.\",\n",
    "    \"Write a song about the intensity of a storm.\",\n",
    "    \"Write a song about growing old with someone.\",\n",
    "    \"Write a song about the colors of autumn.\",\n",
    "    \"Write a song about the loss of innocence.\",\n",
    "    \"Write a song about finding strength within.\",\n",
    "    \"Write a song about the power of forgiveness.\",\n",
    "    \"Write a song about escaping from reality.\",\n",
    "    \"Write a song about a moment of unexpected kindness.\",\n",
    "    \"Write a song about the mysteries of the ocean.\",\n",
    "    \"Write a song about a reunion after many years.\",\n",
    "    \"Write a song about the heartache of saying goodbye.\",\n",
    "    \"Write a song about a childhood memory.\",\n",
    "    \"Write a song about the adrenaline of competition.\",\n",
    "    \"Write a song about a betrayal by someone trusted.\",\n",
    "    \"Write a song about the joy of a newborn's first laugh.\",\n",
    "    \"Write a song about the struggle for justice.\",\n",
    "    \"Write a song about a night out with friends.\",\n",
    "    \"Write a song about dealing with inner demons.\",\n",
    "    \"Write a song about a journey across the world.\",\n",
    "    \"Write a song about the calm before a storm.\",\n",
    "    \"Write a song about the exhilaration of a first kiss.\",\n",
    "    \"Write a song about the sorrow of a grave mistake.\",\n",
    "    \"Write a song about the beauty of a sunrise.\",\n",
    "    \"Write a song about the challenges of parenthood.\",\n",
    "    \"Write a song about finding a letter from the past.\",\n",
    "    \"Write a song about the conflict between heart and mind.\",\n",
    "    \"Write a song about the comfort of old friendships.\",\n",
    "    \"Write a song about the sting of harsh truths.\",\n",
    "    \"Write a song about a day spent in nature.\",\n",
    "    \"Write a song about a vow of eternal loyalty.\",\n",
    "    \"Write a song about the chaos of a city life.\",\n",
    "    \"Write a song about longing for distant places.\",\n",
    "    \"Write a song about the peace of a snowy day.\",\n",
    "    \"Write a song about breaking free from constraints.\",\n",
    "    \"Write a song about the thrill of a chase.\",\n",
    "    \"Write a song about the warmth of a fireside gathering.\",\n",
    "    \"Write a song about the pain of parting ways.\",\n",
    "    \"Write a song about a secret kept for years.\",\n",
    "    \"Write a song about the joy of a festival.\",\n",
    "    \"Write a song about a love that could have been.\",\n",
    "    \"Write a song about an unexpected encounter.\",\n",
    "    \"Write a song about a promise made long ago.\",\n",
    "    \"Write a song about the relief of a confession.\",\n",
    "    \"Write a song about the struggle to belong.\",\n",
    "    \"Write a song about an unforgettable day at the beach.\",\n",
    "    \"Write a song about a journey on a train.\",\n",
    "    \"Write a song about a lesson learned the hard way.\",\n",
    "    \"Write a song about the bliss of a lazy day.\",\n",
    "    \"Write a song about a fight for a cause.\",\n",
    "    \"Write a song about reconnecting with an old flame.\",\n",
    "    \"Write a song about the agony of a tough decision.\",\n",
    "    \"Write a song about a heroic deed.\",\n",
    "    \"Write a song about the discovery of a secret world.\",\n",
    "    \"Write a song about a night of wild festivities.\",\n",
    "    \"Write a song about a lifelong friendship.\",\n",
    "    \"Write a song about a road trip with friends.\",\n",
    "    \"Write a song about a mysterious stranger.\",\n",
    "    \"Write a song about the fear of the unknown.\",\n",
    "    \"Write a song about the thrill of victory.\",\n",
    "    \"Write a song about the despair of defeat.\",\n",
    "    \"Write a song about a moment of clarity.\",\n",
    "    \"Write a song about the beauty of falling leaves.\",\n",
    "    \"Write a song about the tension of a rivalry.\",\n",
    "    \"Write a song about the bittersweet end of a journey.\",\n",
    "    \"Write a song about the anticipation of a reunion.\",\n",
    "    \"Write a song about a leap of faith.\",\n",
    "    \"Write a song about the sorrow of unfulfilled dreams.\",\n",
    "    \"Write a song about the joy of a surprise.\",\n",
    "    \"Write a song about the weight of responsibility.\",\n",
    "    \"Write a song about the thrill of the unknown.\",\n",
    "    \"Write a song about a walk in the moonlight.\",\n",
    "    \"Write a song about the shock of a sudden change.\",\n",
    "    \"Write a song about the comfort of a familiar song.\",\n",
    "    \"Write a song about the pain of a harsh reality.\",\n",
    "    \"Write a song about the excitement of a new beginning.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lyrics B', 'Lyrics B', 'Lyrics B', 'Lyrics B', 'Lyrics B', 'Lyrics A', 'Lyrics B', 'Lyrics A', 'Lyrics A', 'Lyrics B', 'Lyrics A', 'Lyrics A', 'Lyrics A', 'Lyrics B', 'Lyrics B', 'Lyrics B', 'Lyrics B', 'Lyrics B', 'Lyrics A', 'Lyrics B', 'Lyrics A', 'Lyrics A', 'Lyrics B', 'Lyrics B', 'Lyrics B', 'Lyrics B', 'Lyrics B', 'Lyrics A', 'Lyrics A', 'Lyrics B', 'Lyrics B', 'Lyrics A', 'Lyrics B', 'Lyrics B', 'Lyrics A', 'Lyrics A', 'Lyrics B', 'Lyrics B', 'Lyrics B', 'Lyrics B', 'Lyrics A', 'Lyrics A', 'Lyrics A', 'Lyrics A', 'Lyrics A', 'Lyrics B', 'Lyrics A', 'Lyrics B', 'Lyrics B', 'Lyrics B', 'Lyrics B', 'Lyrics A', 'Lyrics B', 'Lyrics B', 'Lyrics B', 'Lyrics B', 'Lyrics B', 'Lyrics B', 'Lyrics A', 'Lyrics A', 'Lyrics A', 'Lyrics B', 'Lyrics B', 'Lyrics A', 'Lyrics A', 'Lyrics A', 'Lyrics B', 'Lyrics A', 'Lyrics A', 'Lyrics B', 'Lyrics A', 'Lyrics A', 'Lyrics B', 'Lyrics A', 'Lyrics B', 'Lyrics B', 'Lyrics A', 'Lyrics B', 'Lyrics B', 'Lyrics A', 'Lyrics B', 'Lyrics A', 'Lyrics B', 'Lyrics B', 'Lyrics A', 'Lyrics A', 'Lyrics B', 'Lyrics A', 'Lyrics B', 'Lyrics A', 'Lyrics B']\n",
      "Our Model was chosen 57.14285714285714% of the time.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import logging\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"OPENAI_API_KEY\",\n",
    ")\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "def compare_lyrics(lyrics1, lyrics2):\n",
    "    prompt_text = f\"Here are two sets of song lyrics:\\n\\nLyrics A:\\n{lyrics1}\\n\\nLyrics B:\\n{lyrics2}\\n\\nWhich set of lyrics do you think is better (for your answer, just put your response ex. Lyrics _)?\"\n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_text}],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "### gpt model stuff\n",
    "model_base = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "### fine tuned model stuff\n",
    "checkpoint_path = './results'\n",
    "model_fine_tuned = GPT2LMHeadModel.from_pretrained(checkpoint_path)\n",
    "tokenizer_fine_tuned = GPT2Tokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "text_generator_base = pipeline('text-generation', model=model_base, tokenizer=tokenizer)\n",
    "text_generator_finetuned = pipeline('text-generation', model=model_fine_tuned, tokenizer=tokenizer_fine_tuned)\n",
    "results = []\n",
    "\n",
    "for prompt in song_prompts[:3]:\n",
    "    generated_lyrics_finetuned = text_generator_finetuned(prompt, max_length=500, truncation=True)[0]['generated_text']\n",
    "    generated_lyrics_base = text_generator_base(prompt, max_length=500, truncation=True)[0]['generated_text']\n",
    "    result = compare_lyrics(generated_lyrics_base, generated_lyrics_finetuned)\n",
    "    results.append(result)\n",
    "    \n",
    "print(results)\n",
    "\n",
    "choices_count = np.mean([r == 'Lyrics B' for r in results])\n",
    "print(f\"Our Model was chosen {choices_count * 100}% of the time.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics A',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics A',\n",
       " 'Lyrics A',\n",
       " 'Lyrics A',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics A',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics A',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B',\n",
       " 'Lyrics A',\n",
       " 'Lyrics B']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
